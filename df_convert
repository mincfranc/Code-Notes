{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data2.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU3/HHE.D_Partial'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of files found: 29\n",
      "\n",
      "Found 29 files in /notebooks/Mine-folder/output/FWU2/HHE.D\n",
      "Batch 1 processed.\n",
      "Batch 2 processed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    " import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data7.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU2/HHE.D'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of files found: 29\n",
      "\n",
      "Found 29 files in /notebooks/Mine-folder/output/FWU3/HHZ.D_Partial\n",
      "Batch 1 processed.\n",
      "Batch 2 processed.\n",
      "Batch 3 processed.\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29 entries, 0 to 28\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   network        29 non-null     object \n",
      " 1   station        29 non-null     object \n",
      " 2   location       29 non-null     object \n",
      " 3   channel        29 non-null     object \n",
      " 4   starttime      29 non-null     object \n",
      " 5   endtime        29 non-null     object \n",
      " 6   sampling_rate  29 non-null     float64\n",
      " 7   data           29 non-null     object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 1.9+ KB\n",
      "None\n",
      "  network station location channel            starttime  \\\n",
      "0      ZZ    FWU3       10     HHZ  2019-12-17T00:00:00   \n",
      "1      ZZ    FWU3       10     HHZ  2019-12-13T00:00:00   \n",
      "2      ZZ    FWU3       10     HHZ  2019-12-26T00:00:00   \n",
      "3      ZZ    FWU3       10     HHZ  2019-11-09T00:00:00   \n",
      "4      ZZ    FWU3       10     HHZ  2019-11-21T00:00:00   \n",
      "\n",
      "                      endtime  sampling_rate  \\\n",
      "0  2019-12-17T23:59:59.996000          250.0   \n",
      "1  2019-12-13T23:59:59.996000          250.0   \n",
      "2  2019-12-26T23:59:59.996000          250.0   \n",
      "3  2019-11-09T23:59:59.996000          250.0   \n",
      "4  2019-11-21T23:59:59.996000          250.0   \n",
      "\n",
      "                                                data  \n",
      "0  [1701, -1159, -4140, -5789, -5853, -3891, -703...  \n",
      "1  [2205, -490, -1743, -7069, -4607, 4999, 3136, ...  \n",
      "2  [-2305, 504, 1486, -1885, -1369, 2413, -671, -...  \n",
      "3  [277, -176, -306, 661, 1270, 823, 103, -594, 7...  \n",
      "4  [-194, -299, -387, -258, 44, 129, 126, 140, -4...  \n",
      "   network station location channel            starttime  \\\n",
      "24      ZZ    FWU3       10     HHZ  2019-11-05T00:00:00   \n",
      "25      ZZ    FWU3       10     HHZ  2019-11-20T00:00:00   \n",
      "26      ZZ    FWU3       10     HHZ  2019-11-15T00:00:00   \n",
      "27      ZZ    FWU3       10     HHZ  2019-12-27T00:00:00   \n",
      "28      ZZ    FWU3       10     HHZ  2019-11-29T00:00:00   \n",
      "\n",
      "                       endtime  sampling_rate  \\\n",
      "24  2019-11-05T18:49:16.284000          250.0   \n",
      "25  2019-11-20T23:59:59.996000          250.0   \n",
      "26  2019-11-15T23:59:59.996000          250.0   \n",
      "27  2019-12-27T01:55:33.660000          250.0   \n",
      "28  2019-11-29T23:59:59.996000          250.0   \n",
      "\n",
      "                                                 data  \n",
      "24  [7693, -7872, -10247, -1716, 2217, 9557, 9438,...  \n",
      "25  [2307, 199, -1986, -3334, -3626, -2496, -4, 23...  \n",
      "26  [-184, 19, 157, 48, -27, 42, 53, 4, 100, 181, ...  \n",
      "27  [1768, -774, -5083, 176, 5973, 5222, -3010, -2...  \n",
      "28  [165, 138, 149, 300, 326, 287, 215, 23, -140, ...  \n",
      "\n",
      "Data Statistics:\n",
      "       sampling_rate\n",
      "count           29.0\n",
      "mean           250.0\n",
      "std              0.0\n",
      "min            250.0\n",
      "25%            250.0\n",
      "50%            250.0\n",
      "75%            250.0\n",
      "max            250.0\n",
      "Failed to merge DataFrames.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_merge_parquet_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all parquet files from the specified directory and its subdirectories,\n",
    "    merges them into a single DataFrame, and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing parquet files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame object representing concatenated data from all parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrame objects\n",
    "    dataframes = []\n",
    "\n",
    "    # Function to find all files recursively\n",
    "    def find_all_files(directory):\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        return all_files\n",
    "\n",
    "    # Find all Parquet files in the directory structure\n",
    "    all_files = find_all_files(directory)\n",
    "\n",
    "    print(f\"\\nTotal number of files found: {len(all_files)}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\\nNo files found in any directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(all_files)} files in {directory}\")\n",
    "\n",
    "    # Process files in batches\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        try:\n",
    "            batch_files = all_files[i:i + batch_size]\n",
    "            batch = [pd.read_parquet(filepath) for filepath in batch_files]\n",
    "            dataframes.extend(batch)\n",
    "            print(f\"Batch {i//batch_size + 1} processed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    result_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display information about the resulting DataFrame\n",
    "    print(\"\\nDataFrame Information:\")\n",
    "    print(result_df.info())\n",
    "    print(result_df.head())\n",
    "    print(result_df.tail())\n",
    "    \n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(result_df.describe())\n",
    "\n",
    "    return result_df.to_csv('merged_seismic_data6.csv')\n",
    "\n",
    "# Usage\n",
    "directory = '/notebooks/Mine-folder/output/FWU3/HHZ.D_Partial'\n",
    "merged_dataframe = read_and_merge_parquet_files(directory)\n",
    "\n",
    "if merged_dataframe is not None:\n",
    "    print(\"Merged DataFrame shape:\", merged_dataframe.shape)\n",
    "else:\n",
    "    print(\"Failed to merge DataFrames.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
